DSP spread issues start here. Appears all nodes have some issues scheduling dsp pods at first then it appears to only be the case on c0n2.
*/auto/bugs/AS-204840/2025-06-09_08-11-49/test_c57640913/step.log*
```
2025-06-09 08:13:34,267 1252798 minas.model.home_fleet_kubernetes_cluster WARNING __init__ test_c57640913[C57640913] Problem (likely temporary) checking DSP pod spread.  Continuing...
2025-06-09 08:13:53,478 1252798 minas.model.home_fleet_kubernetes_cluster WARNING __init__ test_c57640913[C57640913] No pods found!
.....
2025-06-09 08:33:13,994 1252798 minas.model.home_fleet_kubernetes_cluster WARNING __init__ test_c57640913[C57640913] No pods found!
2025-06-09 08:33:13,994 1252798 minas.model.home_fleet_kubernetes_cluster.hf_node WARNING __init__ test_c57640913[C57640913] No active dsp pods found on node c0n2!
.....
2025-06-09 08:42:34,879 1252798 root ERROR tlogger test_c57640913[C57640913] Exception Raised!
error message: borg.model.exceptions.ValidationFailureException: method name: wait_until_dsp_pods_spread_normalized
error message: Timeout waiting for DSP pods spread to be normalized across nodes.
path: /data/workspace/mikaj/venv2/lib64/python3.8/site-packages/minas/model/home_fleet_kubernetes_cluster/__init__.py
line: 1264
E   borg.model.exceptions.ValidationFailureException: method name: wait_until_dsp_pods_spread_normalized
    error message: Timeout waiting for DSP pods spread to be normalized across nodes.
2025-06-09 08:42:34,881 1252798 root STEP tresults test_c57640913[C57640913] --------------------------------------------------
2025-06-09 08:42:34,882 1252798 root STEP tresults test_c57640913[C57640913] TEST RESULT: ERROR
2025-06-09 08:42:34,882 1252798 root STEP tresults test_c57640913[C57640913] --------------------------------------------------
```

The dspmapper logs show that all nodes are eventually ready but not before the test times out. It appears that c0n2 became data ready just before the test times out. There wasn't enough time for the descheduler to remove dsp pods from the other nodes so DSPs could be scheduled on c0n2 and hopefully be balanced across the cluster. The c0n2 node seems to be the node that took the longest, as well as the node that fell outside of the timeout so I will look into what caused it to be data not ready. This shows the order the nodes become data ready.

c0n3 is data ready at 14:12:38
c0n4 is data ready at 14:13:42
c0n7 is data ready at 14:13:42
c0n0 is data ready at 14:15:52
c0n1 is data ready at 14:19:42
c0n5 is data ready at 14:29:02
c0n6 is data ready at 14:29:02
c0n2 is data ready at 14:42:02

*./acm/artifacts_20250609_101717_314347/dna_decompress/hfsim-rd-cluster/log.250609.163720.0001/c0n4/var/log/pods/ds_dspmapper-744b8f4df5-s2fdq_3afe35d8-2d09-45a0-a38a-607d4c779594/dspmapper/0.log*
```
2025-06-09T14:12:38.327347576Z stdout F [INFO] 2025-06-09,14:12:38.327182Z k8snodes.go:392] Going to queue node c0n3 event {nodes K8_NODE_DATA_READY c0n3} for processing
2025-06-09T14:12:38.32737656Z stdout F [DEBUG] 2025-06-09,14:12:38.327274Z event_manager.go:207] Time taken to process {nodes K8_NODE_DATA_READY c0n3} event, {Instance count: 9, Current elapsed: 5.56µs, Average elapsed: 32.20221332s}
.....
2025-06-09T14:13:42.24573216Z stdout F [INFO] 2025-06-09,14:13:42.245523Z k8snodes.go:392] Going to queue node c0n4 event {nodes K8_NODE_DATA_READY c0n4} for processing
2025-06-09T14:13:42.245780682Z stdout F [DEBUG] 2025-06-09,14:13:42.245616Z event_manager.go:207] Time taken to process {nodes K8_NODE_DATA_READY c0n4} event, {Instance count: 10, Current elapsed: 5.089µs, Average elapsed: 28.981992497s}
.....
2025-06-09T14:13:42.251877066Z stdout F [INFO] 2025-06-09,14:13:42.251793Z k8snodes.go:392] Going to queue node c0n7 event {nodes K8_NODE_DATA_READY c0n7} for processing
2025-06-09T14:13:42.279039438Z stdout F [DEBUG] 2025-06-09,14:13:42.278935Z event_manager.go:207] Time taken to process {nodes K8_NODE_DATA_READY c0n7} event, {Instance count: 11, Current elapsed: 27.073225ms, Average elapsed: 26.349727109s}
.....
2025-06-09T14:15:52.246377152Z stdout F [INFO] 2025-06-09,14:15:52.246157Z k8snodes.go:392] Going to queue node c0n0 event {nodes K8_NODE_DATA_READY c0n0} for processing
2025-06-09T14:15:52.246425462Z stdout F [DEBUG] 2025-06-09,14:15:52.24626Z event_manager.go:207] Time taken to process {nodes K8_NODE_DATA_READY c0n0} event, {Instance count: 12, Current elapsed: 7.373µs, Average elapsed: 24.153917131s}
.....
2025-06-09T14:19:42.196868955Z stdout F [INFO] 2025-06-09,14:19:42.196692Z k8snodes.go:392] Going to queue node c0n1 event {nodes K8_NODE_DATA_READY c0n1} for processing
2025-06-09T14:19:42.196933867Z stdout F [DEBUG] 2025-06-09,14:19:42.19681Z event_manager.go:207] Time taken to process {nodes K8_NODE_DATA_READY c0n1} event, {Instance count: 13, Current elapsed: 11.532µs, Average elapsed: 22.295924392s}
.....
2025-06-09T14:29:02.201178003Z stdout F [INFO] 2025-06-09,14:29:02.200966Z k8snodes.go:392] Going to queue node c0n5 event {nodes K8_NODE_DATA_READY c0n5} for processing
2025-06-09T14:29:02.201221675Z stdout F [DEBUG] 2025-06-09,14:29:02.201094Z event_manager.go:207] Time taken to process {nodes K8_NODE_DATA_READY c0n5} event, {Instance count: 14, Current elapsed: 5.751µs, Average elapsed: 20.703358775s}
.....
2025-06-09T14:29:02.226101493Z stdout F [INFO] 2025-06-09,14:29:02.222778Z k8snodes.go:392] Going to queue node c0n6 event {nodes K8_NODE_DATA_READY c0n6} for processing
2025-06-09T14:29:02.238470595Z stdout F [DEBUG] 2025-06-09,14:29:02.238391Z event_manager.go:207] Time taken to process {nodes K8_NODE_DATA_READY c0n6} event, {Instance count: 15, Current elapsed: 15.525818ms, Average elapsed: 19.324169911s}
.....
2025-06-09T14:42:02.285550742Z stdout F [INFO] 2025-06-09,14:42:02.285358Z k8snodes.go:392] Going to queue node c0n2 event {nodes K8_NODE_DATA_READY c0n2} for processing
2025-06-09T14:42:02.285626654Z stdout F [DEBUG] 2025-06-09,14:42:02.285452Z event_manager.go:207] Time taken to process {nodes K8_NODE_DATA_READY c0n2} event, {Instance count: 16, Current elapsed: 8.526µs, Average elapsed: 18.116409825s}
```


Descheduler logs show the  DSP pods can fit on c0n2 at 14:43:02 and pod evictions to fulfil topology thread constraints soon after. This occurs about 30 seconds after the test failure. 

*./acm/artifacts_20250609_101717_314347/dna_decompress/hfsim-rd-cluster/log.250609.163720.0001/c0n2/var/log/pods/ds_descheduler-67775d649c-6nkfr_3d3fa74d-0cdd-47aa-8031-662fb73e2c53/descheduler/0.log*
```
2025-06-09T14:43:02.327805715Z stderr F I0609 14:43:02.327754       1 topologyspreadconstraint.go:143] "Processing namespace for topology spread constraints" namespace="ds-dsp"
2025-06-09T14:43:02.328327314Z stderr F I0609 14:43:02.328279       1 node.go:154] "Pod fits on node" pod="ds-dsp/dsp-874-db758649c-dnfsr" node="c0n2"
2025-06-09T14:43:02.328421399Z stderr F I0609 14:43:02.328375       1 node.go:154] "Pod fits on node" pod="ds-dsp/dsp-10771-5d9956f888-bpffj" node="c0n2"
2025-06-09T14:43:02.328506668Z stderr F I0609 14:43:02.328469       1 node.go:154] "Pod fits on node" pod="ds-dsp/dsp-10771-5d9956f888-bpffj" node="c0n2"
2025-06-09T14:43:02.328607034Z stderr F I0609 14:43:02.328554       1 node.go:154] "Pod fits on node" pod="ds-dsp/dsp-10771-5d9956f888-bpffj" node="c0n2"
2025-06-09T14:43:02.32867476Z stderr F I0609 14:43:02.328641       1 node.go:154] "Pod fits on node" pod="ds-dsp/dsp-10771-5d9956f888-bpffj" node="c0n2"
2025-06-09T14:43:02.328767893Z stderr F I0609 14:43:02.328729       1 node.go:154] "Pod fits on node" pod="ds-dsp/dsp-18439-cd9fc7446-fj8b2" node="c0n2"
2025-06-09T14:43:02.328853482Z stderr F I0609 14:43:02.328817       1 node.go:154] "Pod fits on node" pod="ds-dsp/dsp-61440-7d74ff4959-dj8qg" node="c0n2"
2025-06-09T14:43:02.328971161Z stderr F I0609 14:43:02.328932       1 node.go:154] "Pod fits on node" pod="ds-dsp/dsp-10771-5d9956f888-bpffj" node="c0n2"
2025-06-09T14:43:02.329055688Z stderr F I0609 14:43:02.329021       1 node.go:154] "Pod fits on node" pod="ds-dsp/dsp-10771-5d9956f888-bpffj" node="c0n2"
2025-06-09T14:43:02.329143932Z stderr F I0609 14:43:02.329107       1 node.go:154] "Pod fits on node" pod="ds-dsp/dsp-18439-cd9fc7446-fj8b2" node="c0n2"
2025-06-09T14:43:02.329248636Z stderr F I0609 14:43:02.329192       1 node.go:154] "Pod fits on node" pod="ds-dsp/dsp-10771-5d9956f888-bpffj" node="c0n2"
2025-06-09T14:43:02.329335909Z stderr F I0609 14:43:02.329290       1 node.go:154] "Pod fits on node" pod="ds-dsp/dsp-18439-cd9fc7446-fj8b2" node="c0n2"
2025-06-09T14:43:02.345888068Z stderr F I0609 14:43:02.345795       1 evictions.go:170] "Evicted pod" pod="ds-dsp/dsp-10771-5d9956f888-bpffj" reason="" strategy="RemovePodsViolatingTopologySpreadConstraint" node="c0n1" profile="strategy-RemovePodsViolatingTopologySpreadConstraint-profile"
2025-06-09T14:43:02.359192322Z stderr F I0609 14:43:02.359103       1 evictions.go:170] "Evicted pod" pod="ds-dsp/dsp-18439-cd9fc7446-fj8b2" reason="" strategy="RemovePodsViolatingTopologySpreadConstraint" node="c0n5" profile="strategy-RemovePodsViolatingTopologySpreadConstraint-profile"
2025-06-09T14:43:02.372620406Z stderr F I0609 14:43:02.372539       1 evictions.go:170] "Evicted pod" pod="ds-dsp/dsp-61440-7d74ff4959-dj8qg" reason="" strategy="RemovePodsViolatingTopologySpreadConstraint" node="c0n6" profile="strategy-RemovePodsViolatingTopologySpreadConstraint-profile"
2025-06-09T14:43:02.429587429Z stderr F I0609 14:43:02.429404       1 evictions.go:170] "Evicted pod" pod="ds-dsp/dsp-874-db758649c-dnfsr" reason="" strategy="RemovePodsViolatingTopologySpreadConstraint" node="c0n0" profile="strategy-RemovePodsViolatingTopologySpreadConstraint-profile"
```

NPD logs show c0n2 becoming data not ready at 14:06:13 and not recovering for over 30 min until 14:41:51.

*./acm/artifacts_20250609_101717_314347/dna_decompress/hfsim-rd-cluster/log.250609.163720.0001/c0n2/var/log/pods/ds_npd-db52a52e-9cb86_2e8cec6f-9337-4ce0-a216-26521e714664/node-problem-detector/0.log*
```
2025-06-09T14:06:13.743928915Z stderr F I0609 14:06:13.743768       1 custom_plugin_monitor.go:280] New status generated: &{Source:data-ready-check Events:[{Severity:warn Timestamp:2025-06-09 14:06:13.743712412 +0000 UTC m=+2.521865121 Reason:NodeDataNotReady Message:Node condition DataNotReady is now: True, reason: NodeDataNotReady, message: "key with node lease does not exist"}] Conditions:[{Type:DataNotReady Status:True Transition:2025-06-09 14:06:13.743712412 +0000 UTC m=+2.521865121 Reason:NodeDataNotReady Message:key with node lease does not exist}]}
.....
2025-06-09T14:41:51.273677483Z stderr F I0609 14:41:51.273533       1 custom_plugin_monitor.go:280] New status generated: &{Source:data-ready-check Events:[{Severity:info Timestamp:2025-06-09 14:41:51.273458243 +0000 UTC m=+2140.051377508 Reason:NodeDataReady Message:Node condition DataNotReady is now: False, reason: NodeDataReady, message: "Node is data ready"}] Conditions:[{Type:DataNotReady Status:False Transition:2025-06-09 14:41:51.273458243 +0000 UTC m=+2140.051377508 Reason:NodeDataReady Message:Node is data ready}]}
```

c0n2 leasemanger logs show that leasemanager doesn't get up and running until. What is causing leasemanager to remain in this awaiting initial notification state for so long? Assigning to Mercury for additional triage.

*./acm/artifacts_20250609_101717_314347/dna_decompress/hfsim-rd-cluster/log.250609.163720.0001/c0n2/var/log/pods/cm_leasemanager-0a710a44-6qs2r_2af44896-9a71-406b-9295-d3b815747e77/leasemanager/0.log*
```
2025-06-09T14:07:09.771140097Z stdout F [DEBUG] statemachine.go:386] sm: changing state from "AwaitingWatchdogProbe" to "AwaitingInitialNotification"
2025-06-09T14:07:09.771159273Z stdout F [DEBUG] statemachine.go:408] sm: processing Step() in state "AwaitingInitialNotification"
2025-06-09T14:07:09.771174992Z stdout F [INFO] state.go:473] sm: waiting for the initial notification to be received in state "AwaitingInitialNotification"...
.....
2025-06-09T14:41:43.958046214Z stdout F [DEBUG] statemachine.go:408] sm: processing Step() in state "AwaitingInitialNotification"
2025-06-09T14:41:43.958083002Z stdout F [INFO] state.go:473] sm: waiting for the initial notification to be received in state "AwaitingInitialNotification"...
2025-06-09T14:41:44.223487073Z stdout F [DEBUG] client.go:127] Trying to register client "leasemanager-c0n2" for notifications ([{c0n2 [MAINTENANCE_DAEMONSET_PODS_DEACTIVATE_ENTERING MAINTENANCE_DAEMONSET_PODS_DEACTIVATE_COMPLETED MAINTENANCE_DAEMONSET_PODS_ACTIVATE_COMPLETED]}])
2025-06-09T14:41:44.223505327Z stdout F [INFO] log.go:275] Registering for notifications for resource c0n2 and states [MAINTENANCE_DAEMONSET_PODS_DEACTIVATE_ENTERING MAINTENANCE_DAEMONSET_PODS_DEACTIVATE_COMPLETED MAINTENANCE_DAEMONSET_PODS_ACTIVATE_COMPLETED] as blocking.
2025-06-09T14:41:44.223510567Z stdout F [DEBUG] log.go:265] Registering with server storage-cluster-controller-grpc for resource c0n2 and states [MAINTENANCE_DAEMONSET_PODS_DEACTIVATE_ENTERING MAINTENANCE_DAEMONSET_PODS_DEACTIVATE_COMPLETED MAINTENANCE_DAEMONSET_PODS_ACTIVATE_COMPLETED]
2025-06-09T14:41:44.22760936Z stdout F [DEBUG] log.go:265] Registered with server storage-cluster-controller-grpc for resource c0n2 and states [MAINTENANCE_DAEMONSET_PODS_DEACTIVATE_ENTERING MAINTENANCE_DAEMONSET_PODS_DEACTIVATE_COMPLETED MAINTENANCE_DAEMONSET_PODS_ACTIVATE_COMPLETED]
2025-06-09t14:41:44.227638194z stdout f [info] client.go:135] successfully registered client "leasemanager-c0n2" notifications
2025-06-09t14:41:44.227661928z stdout f [info] statemachine.go:439] sm: successfully registered for notifications
.....
2025-06-09T14:41:44.459012075Z stdout F [DEBUG] statemachine.go:386] sm: changing state from "AwaitingInitialNotification" to "AwaitingInitialPetResult"
```