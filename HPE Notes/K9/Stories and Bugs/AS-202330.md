dsp-mapper logs show that there appears to be an issue with the kube-apiserver. This continues from 00:51:30 to when the logs end at 01:38:52.

*./c0n0/log.250425.015428.0001/c0n0/var/log/pods/ds_dspmapper-65db77fb8d-6r4g4_0b1b8623-f5eb-4318-862f-4814ab20f5e1/dspmapper/0.log*
```
2025-04-25T00:51:30.136764645Z stdout F [DEBUG] 2025-04-25,00:51:30.136406Z k8sclient.go:653] failed to probe API server endpoint, Unauthorized
2025-04-25T00:51:30.136785424Z stdout F [ERROR] 2025-04-25,00:51:30.136474Z health.go:177] health check failed for component dspmapper.k8sconnectivity, {HealthStatus:1 Reason:failed to probe API server endpoint, Unauthorized}
2025-04-25T00:51:30.136788198Z stdout F [INFO] 2025-04-25,00:51:30.136505Z healthservice.go:74] Updating service dspmapper.k8sconnectivity status from SERVING to NOT_SERVING
2025-04-25T00:51:37.400078252Z stdout F [WARN] 2025-04-25,00:51:37.399911Z healthservice.go:44] service dspmapper.k8sconnectivity is not healthy and its status is NOT_SERVING
```

node-problem-detector logs show that c0n0 went data not ready due to missing a key with a node lease at 00:51:14. These logs don't show it ever recovering

*./c0n0/log.250425.015428.0001/c0n0/var/log/pods/ds_npd-df4aa477-m2hhh_dc37cbc4-2dd9-4977-bd52-c204d212278d/node-problem-detector/0.log*
```
2025-04-25T00:51:14.378999596Z stderr F I0425 00:51:14.378877       1 custom_plugin_monitor.go:280] New status generated: &{Source:data-ready-check Events:[{Severity:warn Timestamp:2025-04-25 00:51:14.378835249 +0000 UTC m=+2440.038901210 Reason:NodeDataNotReady Message:Node condition DataNotReady is now: True, reason: NodeDataNotReady, message: "key with node lease does not exist"}] Conditions:[{Type:DataNotReady Status:True Transition:2025-04-25 00:51:14.378835249 +0000 UTC m=+2440.038901210 Reason:NodeDataNotReady Message:key with node lease does not exist}]}
```

kube-apiserver logs show that the service account tolen has been invalidated so the request can't be authenticated. This is followed up by an HTTP 401

*./c0n1/log.250425.015428.0001/c0n1/var/log/pods/kube-system_kube-apiserver-c0n1_e182fc5d3f6995975a55664aa31ceb97/kube-apiserver/0.log.20250425-010656.gz*```
```
2025-04-25T00:51:29.745650513Z stderr F E0425 00:51:29.745198       1 authentication.go:73] "Unable to authenticate the request" err="[invalid bearer token, service account token has been invalidated, failed to verify jwt, key 'hvmOkMyey-jIZO3PMmwUe42a57dFI_7gl5i4tpSoiGs' not found]"
2025-04-25T00:51:29.745664188Z stderr F I0425 00:51:29.745355       1 httplog.go:132] "HTTP" verb="GET" URI="/livez" latency="44.805731ms" userAgent="dsp/v0.0.0 (linux/amd64) kubernetes/$Format" audit-ID="15343795-a039-4c53-8174-7c60eaad8468" srcIP="172.19.12.20:24688" resp=401
```

Leasemanager logs show that c0n0 became unhealthy at 00:50:56 and never recovered. This is why the node was data not ready, and the DSPs are not distributed evenly across the three nodes. This seems to be caused by the controller runtime being down.

*./c0n0/log.250425.015428.0001/c0n0/var/log/pods/cm_leasemanager-11d13ad5-p2bzv_9a12067b-fa58-4017-9dad-ce69d9c51968/leasemanager/0.log*
```
2025-04-25T00:50:56.759627728Z stdout F [WARN] k8sclient.go:483] Health check "c0n0".status.conditions.type=Ready failed due to condition status not changing within deadline: Status: False (last heartbeat: 2025-04-25 00:49:54 +0000 UTC, last transition: 2025-04-25 00:49:54 +0000 UTC, reason: KubeletNotReady, msg: "container runtime is down", current time: 2025-04-25 00:50:56.759552709 +0000 UTC m=+2386.685392289, status deadline: 2025-04-25 00:50:54 +0000 UTC)
2025-04-25T00:50:56.759647345Z stdout F [WARN] k8sclient.go:509] Node "c0n0" is not healthy
2025-04-25T00:50:56.759673563Z stdout F [WARN] state.go:774] sm: failed K8s node health check in state "AwaitingK8sHealthCheck" (error: <nil>)
2025-04-25T00:50:57.260741669Z stdout F [DEBUG] statemachine.go:408] sm: processing Step() in state "AwaitingK8sHealthCheck"
2025-04-25T00:50:57.260760915Z stdout F [DEBUG] k8sclient.go:426] Checking node health...
2025-04-25T00:50:57.268318664Z stdout F [TRACE] k8sclient.go:435] cli.CoreV1().Nodes().Get() took 7.538935ms
```

Token service logs show a failure to parse and validate the token above starting at 00:51:29 and continuing until 01:18:15 which is the end of the log file. Perhaps the token service not verify the token if the container runtime is down...

*c0n1/log.250425.015428.0001/c0n1/var/log/pods_archive/cm_app-token-service-56c64c7f9d-qpzg8_9275544c-0f88-4c32-bd73-9c47ea59f037/token-service/0.log*
```
2025-04-25T00:51:29.717480218Z stdout F [INFO] 2025-04-25,00:51:29.715504Z token_validation.go:44] validateToken invoked..
2025-04-25T00:51:29.745051566Z stdout F [ERROR] 2025-04-25,00:51:29.74461Z token_validation.go:53] error parsing and verifying token: failed to verify jwt, key 'hvmOkMyey-jIZO3PMmwUe42a57dFI_7gl5i4tpSoiGs' not found
```

Since this appears to be an issue with kubelet, I checked out boot.log on c0n0.

*./c0n0/log.250425.015428.0001/c0n0/var/log/files/boot.log*
```
<30>1 2025-04-25T00:50:52.924145+00:00 c0n0 kubelet 29311 - -  W0425 00:50:52.923999   29311 handler.go:138] failed to obtain memory limit for machine container
<30>1 2025-04-25T00:50:52.935899+00:00 c0n0 kubelet 29311 - -  W0425 00:50:52.931630   29311 handler.go:148] failed to obtain swap limit for machine container
<30>1 2025-04-25T00:50:53.057983+00:00 c0n0 ncs 29912 - -  [INFO] [636f5c45-45c4-452f-9902-0f1da1e273af] [utils.go:259] Request[GET] for API[/api/v3/node/nwinterfaces] Source[172.26.121.157:47772] Target[172.19.12.20:443] has been completed with Status[200]
<30>1 2025-04-25T00:50:54.261121+00:00 c0n0 containerd 27639 - -  time="2025-04-25T00:50:54.259208772Z" level=error msg="Status failed" error="stat /proc/27639/ns/pid: no such file or directory: unknown"
<30>1 2025-04-25T00:50:54.263694+00:00 c0n0 kubelet 29311 - -  E0425 00:50:54.259374   29311 remote_runtime.go:633] "Status from runtime service failed" err="rpc error: code = Unknown desc = stat /proc/27639/ns/pid: no such file or directory: unknown"
<30>1 2025-04-25T00:50:54.263770+00:00 c0n0 kubelet 29311 - -  E0425 00:50:54.259424   29311 kubelet.go:2879] "Container runtime sanity check failed" err="rpc error: code = Unknown desc = stat /proc/27639/ns/pid: no such file or directory: unknown"
<30>1 2025-04-25T00:50:54.708350+00:00 c0n0 containerd 27639 - -  time="2025-04-25T00:50:54.708218759Z" level=error msg="ExecSync for \"d3f4d3b61e12692c26a44e0d5cd309222d393db6557cd07d9a21dd6665928201\" failed" error="failed to exec in container: failed to create exec \"418ce0b9e97f75c33d93278a705278608ebd1df6c6c8229b6b5b6600eddf729e\": couldn't stat /proc/self/fd/59: stat /proc/self/fd/59: no such file or directory"
<30>1 2025-04-25T00:50:54.708548+00:00 c0n0 kubelet 29311 - -  E0425 00:50:54.708482   29311 remote_runtime.go:496] "ExecSync cmd from runtime service failed" err="rpc error: code = Unknown desc = failed to exec in container: failed to create exec \"418ce0b9e97f75c33d93278a705278608ebd1df6c6c8229b6b5b6600eddf729e\": couldn't stat /proc/self/fd/59: stat /proc/self/fd/59: no such file or directory" containerID="d3f4d3b61e12692c26a44e0d5cd309222d393db6557cd07d9a21dd6665928201" cmd=["/bin/calico-node","-bird-ready","-felix-ready"]
<30>1 2025-04-25T00:50:54.709965+00:00 c0n0 containerd 27639 - -  time="2025-04-25T00:50:54.709909897Z" level=error msg="ExecSync for \"d3f4d3b61e12692c26a44e0d5cd309222d393db6557cd07d9a21dd6665928201\" failed" error="failed to exec in container: failed to create exec \"28f62d4984cb45a7164dc70efe109067ab99d662a009c8e39633b63f01380df6\": couldn't stat /proc/self/fd/59: stat /proc/self/fd/59: no such file or directory"
<30>1 2025-04-25T00:50:54.710126+00:00 c0n0 kubelet 29311 - -  E0425 00:50:54.710087   29311 remote_runtime.go:496] "ExecSync cmd from runtime service failed" err="rpc error: code = Unknown desc = failed to exec in container: failed to create exec \"28f62d4984cb45a7164dc70efe109067ab99d662a009c8e39633b63f01380df6\": couldn't stat /proc/self/fd/59: stat /proc/self/fd/59: no such file or directory" containerID="d3f4d3b61e12692c26a44e0d5cd309222d393db6557cd07d9a21dd6665928201" cmd=["/bin/calico-node","-bird-ready","-felix-ready"]
<30>1 2025-04-25T00:50:54.711840+00:00 c0n0 containerd 27639 - -  time="2025-04-25T00:50:54.711785889Z" level=error msg="ExecSync for \"d3f4d3b61e12692c26a44e0d5cd309222d393db6557cd07d9a21dd6665928201\" failed" error="failed to exec in container: failed to create exec \"63a2b2414eafee5709dd979ddcc61e2a7c31a9b4a7d4f582961e190bfb4ca17f\": couldn't stat /proc/self/fd/59: stat /proc/self/fd/59: no such file or directory"
<30>1 2025-04-25T00:50:54.711988+00:00 c0n0 kubelet 29311 - -  E0425 00:50:54.711953   29311 remote_runtime.go:496] "ExecSync cmd from runtime service failed" err="rpc error: code = Unknown desc = failed to exec in container: failed to create exec \"63a2b2414eafee5709dd979ddcc61e2a7c31a9b4a7d4f582961e190bfb4ca17f\": couldn't stat /proc/self/fd/59: stat /proc/self/fd/59: no such file or directory" containerID="d3f4d3b61e12692c26a44e0d5cd309222d393db6557cd07d9a21dd6665928201" cmd=["/bin/calico-node","-bird-ready","-felix-ready"]
<30>1 2025-04-25T00:50:54.855330+00:00 c0n0 containerd 27639 - -  time="2025-04-25T00:50:54.855131585Z" level=error msg="ExecSync for \"32156f63d4b4e2429e9e8cd2762547e3e243011609764d6f3d2cf8583b6c3cbb\" failed" error="failed to exec in container: failed to create exec \"1e58cf349567e6c32c61b575b62892a73bd628b7426c123dad372f8b9cf7d6b0\": couldn't stat /proc/self/fd/59: stat /proc/self/fd/59: no such file or directory"
<30>1 2025-04-25T00:50:54.855753+00:00 c0n0 kubelet 29311 - -  E0425 00:50:54.855617   29311 remote_runtime.go:496] "ExecSync cmd from runtime service failed" err="rpc error: code = Unknown desc = failed to exec in container: failed to create exec \"1e58cf349567e6c32c61b575b62892a73bd628b7426c123dad372f8b9cf7d6b0\": couldn't stat /proc/self/fd/59: stat /proc/self/fd/59: no such file or directory" containerID="32156f63d4b4e2429e9e8cd2762547e3e243011609764d6f3d2cf8583b6c3cbb" cmd=["/opt/hpe/keyservice/bin/keyservice_probe","readiness"]
<30>1 2025-04-25T00:50:54.857079+00:00 c0n0 containerd 27639 - -  time="2025-04-25T00:50:54.857029868Z" level=error msg="ExecSync for \"32156f63d4b4e2429e9e8cd2762547e3e243011609764d6f3d2cf8583b6c3cbb\" failed" error="failed to exec in container: failed to create exec \"b21492ae9bf9e7b06ce65831235848657d4576d59235a594e97ee0e785b84b1f\": couldn't stat /proc/self/fd/59: stat /proc/self/fd/59: no such file or directory"
<30>1 2025-04-25T00:50:54.857193+00:00 c0n0 kubelet 29311 - -  E0425 00:50:54.857163   29311 remote_runtime.go:496] "ExecSync cmd from runtime service failed" err="rpc error: code = Unknown desc = failed to exec in container: failed to create exec \"b21492ae9bf9e7b06ce65831235848657d4576d59235a594e97ee0e785b84b1f\": couldn't stat /proc/self/fd/59: stat /proc/self/fd/59: no such file or directory" containerID="32156f63d4b4e2429e9e8cd2762547e3e243011609764d6f3d2cf8583b6c3cbb" cmd=["/opt/hpe/keyservice/bin/keyservice_probe","readiness"]
<30>1 2025-04-25T00:50:54.858972+00:00 c0n0 containerd 27639 - -  time="2025-04-25T00:50:54.858876105Z" level=error msg="ExecSync for \"32156f63d4b4e2429e9e8cd2762547e3e243011609764d6f3d2cf8583b6c3cbb\" failed" error="failed to exec in container: failed to create exec \"d7867817948a5837b1ba41beda663a634109740ead9e43fac32aa1bf9a3aaca6\": couldn't stat /proc/self/fd/59: stat /proc/self/fd/59: no such file or directory"
<30>1 2025-04-25T00:50:54.859080+00:00 c0n0 kubelet 29311 - -  E0425 00:50:54.859048   29311 remote_runtime.go:496] "ExecSync cmd from runtime service failed" err="rpc error: code = Unknown desc = failed to exec in container: failed to create exec \"d7867817948a5837b1ba41beda663a634109740ead9e43fac32aa1bf9a3aaca6\": couldn't stat /proc/self/fd/59: stat /proc/self/fd/59: no such file or directory" containerID="32156f63d4b4e2429e9e8cd2762547e3e243011609764d6f3d2cf8583b6c3cbb" cmd=["/opt/hpe/keyservice/bin/keyservice_probe","readiness"]
<30>1 2025-04-25T00:50:54.866473+00:00 c0n0 kubelet 29311 - -  W0425 00:50:54.866187   29311 container.go:586] Failed to update stats for container "/kubepods.slice/kubepods-besteffort.slice/kubepods-besteffort-pod7c9dfa15_fdd3_471b_be71_8ca1e016ad09.slice/cri-containerd-da2f20e317599c01c1154f418c45cc1fde2b49e2cca568e328a23b565f55bd99.scope": error while statting cgroup v2: [cgroupFd 10 unexpectedly opened to  != /sys/fs/cgroup: openat2 /sys/fs/cgroup/kubepods.slice/kubepods-besteffort.slice/kubepods-besteffort-pod7c9dfa15_fdd3_471b_be71_8ca1e016ad09.slice/cri-containerd-da2f20e317599c01c1154f418c45cc1fde2b49e2cca568e328a23b565f55bd99.scope/hugetlb.2MB.rsvd.events: no such file or directory], continuing to push stats
<30>1 2025-04-25T00:50:55.742042+00:00 c0n0 kubelet 29311 - -  W0425 00:50:55.739642   29311 container.go:586] Failed to update stats for container "/kubepods.slice/kubepods-besteffort.slice/kubepods-besteffort-poda49b85c7_21ec_4617_9edc_8af94d0cbf3e.slice/cri-containerd-da79fc358a672a449d15fb2f54161e79027358811dfd55ffc9a71b45be4ff364.scope": error while statting cgroup v2: [cgroupFd 10 unexpectedly opened to  != /sys/fs/cgroup: openat2 /sys/fs/cgroup/kubepods.slice/kubepods-besteffort.slice/kubepods-besteffort-poda49b85c7_21ec_4617_9edc_8af94d0cbf3e.slice/cri-containerd-da79fc358a672a449d15fb2f54161e79027358811dfd55ffc9a71b45be4ff364.scope/hugetlb.2MB.rsvd.events: no such file or directory], continuing to push stats
<30>1 2025-04-25T00:50:56.213538+00:00 c0n0 kubelet 29311 - -  W0425 00:50:56.213384   29311 fs.go:476] Not collecting filesystem statistics because file "/proc/diskstats" was not found
<30>1 2025-04-25T00:50:56.266140+00:00 c0n0 kubelet 29311 - -  I0425 00:50:56.266025   29311 helpers.go:946] "Eviction manager: no observation found for eviction signal" signal="memory.available"
<30>1 2025-04-25T00:50:56.469565+00:00 c0n0 kubelet 29311 - -  E0425 00:50:56.469374   29311 kubelet.go:2355] "Skipping pod synchronization" err="container runtime is down"
```

Since this appears to be an environmental problem I'm assigning this one to simcity for additional triage. Seems to be similar to https://jira.storage.hpecorp.net/browse/AS-198506 or https://jira.storage.hpecorp.net/browse/AS-199834 as the system appeared to be healthy up un 