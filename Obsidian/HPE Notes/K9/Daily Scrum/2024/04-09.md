AS-181025:

7bcc988bb4 --> orphaned replicaset with two running pods lying around - oddly this is the newer one
845756fff6 --> good replicaset that fails to scale up - older one

lir-svc restarts and sees three running lir-data replicas. It initially scales them down to the expected number. What if there are two replicasets between these three replicas. We know when we scale up the deployment the replicaset stays the same; therefore, on this initial scale down we are likely reducing the number of good replicas and the orphaned replicas with the differing replicaset remain.

Still unsure how we get into this state in the first place. Perhaps the lir-data deployment updated when we were right in the middle of scaling up. 

Is there a way to trigger an eviction of previous replicaset running lir-data replicas

Pods with the lower `controller.kubernetes.io/pod-deletion-cost` annotation value are scaled down first


Timeline of replicaset and pod events for lir-data:
05:34:01 -> lir-data-5c77c746b8 replicaset created (iso)
05:43:29 -> lir-data-845756fff6 replicaset created (squashfs)
17:12:08 -> lir-data-7bcc988bb4 replicaset created (update)

17:12:08 -> lir-data-7bcc988bb4-kd624 pod created (Running)
17:12:11 -> lir-data-7bcc988bb4-qbz8s pod created (Running)
18:25:04 -> lir-data-845756fff6-s259l pod created (Running) 
18:25:08 -> lir-data-845756fff6-7v9lq pod created (Pending)

how to check when amit edited the lir-data deployment to change bandwidth limit

make changes so scale down is actually called - closure error



