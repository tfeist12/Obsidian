Still looking into provdmapper image pull backoff bug
	We are likely going to need a repo to determine the root cause
	Posted a comment about fixing the issue if it is a containerd issue vs a lir-data registry issue
	Will continue to learn and attempting to manually force a repo

Found a way to determine what containerd is storing in it's content directories so this will help us
```
	ctr --namespace k8s.io content ls
	ctr --namespace k8s.io containers ls
```

Also want to go ahead and push a review for my hexos based nginx change this week

##### Container Hashes
**Manifest:**
5ce8e2e10d798329a619ab3da31ecea746257bf3e1344074deef1298ca6299dc

**Config:** a42b9f4c53a2bb85d3e6098c63898661d08bb3309c845be0eaf93a251917e5d3

**Blob (Multiple Layers):**
489a1635ba24480d286236135cf98f8ffb682b91564ecd8be66af5ba19943315
6a7110268ab06189ea5bf3bdd7d00b174c53380986b7f1c271e14d1e7ca5ce18
ae9ea9e3d14ccbdcfe5914846158ea5293e2b71b8089000faff966f3e557570b



Container layers in manifest have labels which reference rootfs diff_ids in config

```
spec:
	template:
		spec:
			affinity:
				nodeAffinity:
				  requiredDuringSchedulingIgnoredDuringExecution:
					nodeSelectorTerms:
					- matchExpressions:
					  - key: kubernetes.io/hostname
						operator: NotIn
						values:
						- c0n2
```

If a layer is removed using the ctr command, it is repulled using lir-data registry. It doesn't show up in /var/tmp/containerd/io.containerd.content.v1.content like it did initially though

slack ron about switchd bug

Plan: 
- remove provdmapper replica by reducing replica count to 0
- remove image and content from containerd using crictl rmi --prune a42b9f4c53a2b
- restart containerd using systemctl restart containerd
- restart provdmapper replica and describe the pod to see if it pulled
- check lir-dr logs to show the layer being pulled

This works!